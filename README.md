ğŸ§  Breast Cancer Detection â€” Neural Network From Scratch (NumPy Only)
ğŸš€ Project Overview

This project implements a fully connected neural network (Multi-Layer Perceptron) from first principles, using only Python and NumPy.
No TensorFlow. No PyTorch. No shortcuts.

The model classifies tumors as Malignant or Benign using the Wisconsin Breast Cancer Dataset, proving that deep learning is just math + logic + iteration, not magic.

The goal is simple but ambitious:

Demystify neural networks by building every component manually.

ğŸ§© Why This Project Exists (Read This If Youâ€™re Serious About ML)

Most ML projects hide the learning process behind high-level APIs.
This one does the opposite.

By implementing everything from scratch, this repository demonstrates:

How neural networks actually compute

How gradients flow via calculus (Chain Rule)

How optimization updates weights using linear algebra

Why neural networks are not black boxes if you understand the math

If you can explain this project, you can explain deep learning fundamentals confidently.

ğŸ“Š Dataset

Wisconsin Breast Cancer Dataset

Samples: 569

Features: 30 real-valued tumor characteristics
(radius, texture, smoothness, concavity, symmetry, etc.)

Labels:

0 â†’ Malignant

1 â†’ Benign

All features are standardized to ensure stable gradient descent.

ğŸ—ï¸ Neural Network Architecture
Input Layer (30 features)
        â†“
Hidden Layer (16 neurons)
[Linear â†’ ReLU]
        â†“
Output Layer (1 neuron)
[Linear â†’ Sigmoid]
        â†“
Binary Prediction (0 or 1)

Layer Breakdown

Input Layer:
Receives normalized feature vectors

Hidden Layer:
Learns non-linear feature interactions using ReLU

Output Layer:
Outputs probability of malignancy using Sigmoid



ReLU(z)=max(0,z)

Ïƒ(z)= 1/(1+e^(-x))

	â€‹

2ï¸âƒ£ Loss Function â€” Binary Cross-Entropy

This measures how wrong the prediction is:

ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
[
ğ‘¦
(
ğ‘–
)
log
â¡
(
ğ‘¦
^
(
ğ‘–
)
)
+
(
1
âˆ’
ğ‘¦
(
ğ‘–
)
)
log
â¡
(
1
âˆ’
ğ‘¦
^
(
ğ‘–
)
)
]
L=âˆ’
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

[y
(i)
log(
y
^
	â€‹

(i)
)+(1âˆ’y
(i)
)log(1âˆ’
y
^
	â€‹

(i)
)]

Lower loss = better predictions.

3ï¸âƒ£ Backpropagation (Chain Rule in Action)

Gradients are computed manually for every parameter.

Output Layer Gradient
ğ‘‘
ğ‘
[
2
]
=
ğ´
[
2
]
âˆ’
ğ‘Œ
dZ
[2]
=A
[2]
âˆ’Y
Weight Updates
ğ‘‘
ğ‘Š
[
ğ‘™
]
=
1
ğ‘š
ğ‘‘
ğ‘
[
ğ‘™
]
ğ´
[
ğ‘™
âˆ’
1
]
ğ‘‡
dW
[l]
=
m
1
	â€‹

dZ
[l]
A
[lâˆ’1]
T
Bias Updates
ğ‘‘
ğ‘
[
ğ‘™
]
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
[
ğ‘™
]
db
[l]
=
m
1
	â€‹

âˆ‘dZ
[l]

ReLU derivative:

ğ‘‘
ğ‘‘
ğ‘§
ReLU
(
ğ‘§
)
=
{
1
	
ğ‘§
>
0


0
	
ğ‘§
â‰¤
0
dz
d
	â€‹

ReLU(z)={
1
0
	â€‹

z>0
zâ‰¤0
	â€‹


This is pure calculus + matrix multiplication.

4ï¸âƒ£ Optimization â€” Gradient Descent

Each parameter is updated as:

ğœƒ
:
=
ğœƒ
âˆ’
ğ›¼
â‹…
âˆ‡
ğœƒ
Î¸:=Î¸âˆ’Î±â‹…âˆ‡
Î¸
	â€‹


Where:

ğ›¼
Î± = learning rate

âˆ‡
ğœƒ
âˆ‡
Î¸
	â€‹

 = computed gradient

Repeat this for 1000 epochs, and the network learns.

ğŸ” Training Workflow

Load dataset

Standardize features

Split into train/test sets

Forward propagation

Compute loss

Backpropagation

Update weights

Repeat until convergence

This loop is the heartbeat of deep learning.

ğŸ“ˆ Evaluation Metrics

The model is evaluated using:

Accuracy

Precision

Recall

F1-Score

Sample Results
Training Accuracy: ~98%
Testing Accuracy:  ~96%


High accuracy without overfitting â€” achieved without any DL framework.

ğŸ§ª Example Prediction
Input: [feature vector]
Actual Label: 1 (Benign)
Predicted Label: 1 (Benign)


The model outputs a probability and applies a 0.5 threshold for classification.

ğŸ› ï¸ Tech Stack

Python ğŸ

NumPy (matrix math)

Scikit-learn (data + metrics only)

No deep learning libraries used.

ğŸ¯ What This Project Proves

This repository demonstrates strong understanding of:

Neural network internals

Linear algebra for ML

Gradient-based optimization

Loss functions and activations

End-to-end ML pipelines

In short:
You donâ€™t just use neural networks â€” you understand them.

ğŸ“Œ Future Improvements

Add multi-layer support

Implement Adam optimizer manually

Visualize loss curves

Extend to multiclass classification
